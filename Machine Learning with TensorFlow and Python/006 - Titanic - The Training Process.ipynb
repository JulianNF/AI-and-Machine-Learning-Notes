{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Tensor and Python\n",
    "\n",
    "These are JulianNF's notes from following [freecodecamp's online Machine Learning with Python certification](https://www.freecodecamp.org/learn/machine-learning-with-python), and supplemented by [Google's Tensorflow documentation](https://www.tensorflow.org/guide/tensor)\n",
    "\n",
    "Feel free to benefit from them if you're studying on your own.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "# Required in notebook:\n",
    "%pip install -q sklearn\n",
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np # library for handling arrays better\n",
    "import pandas as pd # library for data manipulation\n",
    "import matplotlib.pyplot as plt # library for graphing\n",
    "from IPython.display import clear_output \n",
    "import urllib\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from previous module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataframe = pd.read_csv(\t'https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
    "testing_dataframe = pd.read_csv(\t'https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\n",
    "\n",
    "training_survived = training_dataframe.pop('survived')\n",
    "testing_survived = testing_dataframe.pop('survived')\n",
    "\n",
    "categorical_columns = [\n",
    "\t'sex',\n",
    "\t'n_siblings_spouses',\n",
    "\t'parch',\n",
    "\t'class',\n",
    "\t'deck',\n",
    "\t'embark_town',\n",
    "\t'alone'\n",
    "]\n",
    "\n",
    "numeric_columns = [\n",
    "\t'age',\n",
    "\t'fare'\n",
    "]\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "for feature_name in categorical_columns:\n",
    "\tvocabulary = training_dataframe[feature_name].unique() # get all unique possible values (aka categories) in the given column\n",
    "\tfeature_columns.append(\n",
    "\t\ttf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)\n",
    "\t)\n",
    "# print(feature_columns)\n",
    "\n",
    "for feature_name in numeric_columns:\n",
    "\tfeature_columns.append(\n",
    "\t\ttf.feature_column.numeric_column(feature_name, dtype=tf.float32)\n",
    "\t)\n",
    "# print(feature_columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Process\n",
    "Typically, the size of the dataset is quite large, and most training processes would not be able to run all the data at once on RAM. We therefore feed our data to our model in smaller batches.\n",
    "\n",
    "In this model, we're going to load the data in batches of 32 data points. Notes that feeding data to our model 1 data point at a time would actually be slower than doing it in \"bit size chunks\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs\n",
    "As we feed more and more batches of data into our model, it will improve. However, we need to feed the data multiple times so that the model can process all data points along with all the other data points. \n",
    "\n",
    "❓TBC - sounds like the number of epochs is related to all the permutations required so that all the data points have been processed alongside all the other datapoints.\n",
    "\n",
    "Each epoch is therefore equal to one complete stream of our data. The number of epochs we feed into our model will be equal to the number of times that our model sees the complete data set.\n",
    "\n",
    "❓TBC - An epoch is therefore \"one round of training\"?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Function\n",
    "We need to be able to convert our Pandas dataframe to a TensorFlow dataset (`tf.data.Dataset`). To do this, we need to create an input function.\n",
    "\n",
    "Here's an example from the TensorFlow documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_function(data_dataframe, label_dataframe, num_epochs=10, shuffle=True, batch_size=32):\n",
    "\tdef input_function():\n",
    "\t\t# create a labeled TF dataset from our dataframe:\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices(\n",
    "\t\t\tdict(data_dataframe),\n",
    "\t\t\tlabel_dataframe\n",
    "\t\t)\n",
    "\t\tif shuffle:\n",
    "\t\t\tdataset = dataset.shuffle(1000)\n",
    "\t\t# split dataset into batches and repeat for as many epochs as requested:\n",
    "\t\tdataset = dataset.batch(batch_size).repeat(num_epochs)\n",
    "\t\treturn dataset\n",
    "\treturn input_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Our input function generator has default values for number of epochs, shuffling, and batch size:\n",
    "training_input_function = make_input_function(training_dataframe, training_survived)\n",
    "\n",
    "# NB: For our testing function, we only need one epoch and no shuffling:\n",
    "testing_input_function = make_input_function(testing_dataframe, testing_survived, num_epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Julian\\AppData\\Local\\Temp\\tmpu5etxabp\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\Julian\\\\AppData\\\\Local\\\\Temp\\\\tmpu5etxabp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Create a linear estimator model:\n",
    "linear_estimator = tf.estimator.LinearClassifier(feature_columns=feature_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4b873e07bceb12c2b9a42b36e7722015039dfeed477cfe3ebf1d2c97eb81cf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
